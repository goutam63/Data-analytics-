# Data Analysis and Visualisation
The collection, transformation, and organization of data to draw conclusions make predictions for the future, and make informed data-driven decisions is called Data Analysis. The profession that handles data analysis is called a Data Analyst.There is a huge demand for Data Analysts as the data is expanding rapidly nowadays. Data Analysis is used to find possible solutions for a business problem. The advantage of being a Data Analyst is that they can work in any field they love: healthcare, agriculture, IT, finance, business. Data-driven decision-making is an important part of Data Analysis. It makes the analysis process much easier. 

## There are six steps for Data

Ask or Specify Data Requirements

Prepare or Collect Data

Clean and Process

Analyze 

Share

Act or Report

## Ask or Specify Data Requirements
The first step in the process is to Ask. The data analyst is given a problem/business task. The analyst has to understand the task and the stakeholder’s expectations for the solution. A stakeholder is a person that has invested their money and resources to a project. The analyst must be able to ask different questions in order to find the right solution to their problem. The analyst has to find the root cause of the problem in order to fully understand the problem. The analyst must make sure that he/she doesn’t have any distractions while analyzing the problem. Communicate effectively with the stakeholders and other colleagues to completely understand what the underlying problem is. Questions to ask yourself for the Ask phase are: 

What are the problems that are being mentioned by my stakeholders?
What are their expectations for the solutions?

## Prepare of collect Data 

The second step is to Prepare or Collect the Data. This step includes collecting data and storing it for further analysis. The analyst has to collect the data based on the task given from multiple sources. The data has to be collected from various sources, internal or external sources. Internal data is the data available in the organization that you work for while external data is the data available in sources other than your organization. The data that is collected by an individual from their own resources is called first-party data. The data that is collected and sold is called second-party data. Data that is collected from outside sources is called third-party data. The common sources from where the data is collected are Interviews, Surveys, Feedbacks, Questionnaires. The collected data can be stored in a spreadsheet or SQL database. 

A spreadsheet is a digital worksheet that contains rows and columns while a database contains tables that have functions to manipulate the data. Spreadsheets are used to store some thousands or ten thousand of data while databases are used when there are too many rows to store. The best tools to store the data are MS Excel or Google Sheets in the case of Spreadsheets and there are so many databases like Oracle, Microsoft to store the data.

## Clean and Process Data

The third step is Process. After the data is collected from multiple sources, it is time to clean the data. Clean data means data that is free from misspellings, redundancies, and irrelevance. Clean data largely depends on data integrity. There might be duplicate data or the data might not be in a format, therefore the unnecessary data is removed and cleaned. There are different functions provided by SQL and Excel to clean the data. This is one of the most important steps in Data Analysis as clean and formatted data helps in finding trends and solutions. The most important part of the Process phase is to check whether your data is biased or not. Bias is an act of favoring a particular group/community while ignoring the rest. Biasing is a big no-no as it might affect the overall data analysis. The data analyst must make sure to include every group while the data is being collected. 

## Analyze

The fourth step is to Analyze. The cleaned data is used for analyzing and identifying trends. It also performs calculations and combines data for better results. The tools used for performing calculations are Excel or SQL. These tools provide in-built functions to perform calculations or sample code is written in SQL to perform calculations. Using Excel, we can create pivot tables and perform calculations while SQL creates temporary tables to perform calculations. Programming languages are another way of solving problems. They make it much easier to solve problems by providing packages. The most widely used programming languages for data analysis are R and Python.

## Share

The fifth step is Share. Nothing is more compelling than a visualization. The data now transformed has to be made into a visual(chart, graph). The reason for making data visualizations is that there might be people, mostly stakeholders that are non-technical. Visualizations are made for a simple understanding of complex data. Tableau and Looker are the two popular tools used for compelling data visualizations. Tableau is a simple drag and drop tool that helps in creating compelling visualizations. Looker is a data viz tool that directly connects directly to the database and creates visualizations. Tableau and Looker are both equally used by data analysts for creating a visualization. R and Python have some packages that provide beautiful data visualizations. R has a package named ggplot which has a variety of data visualizations. A presentation is given based on the data findings. Sharing the insights with the team members and stakeholders will help in making better decisions. It helps in making more informed decisions and it leads to better outcomes. 

## Act or Report

The final/sixth step is Act. After a presentation is given based on your findings, the stakeholders discuss whether to move forward or not. If they agreed to your recommendations, they move further with your solutions. If they don’t agree with your findings,  you will have to dig deeper to find more possible solutions. Every step has to be re-organized. We have to repeat every step to see whether there are any gaps in there. The data collected must be reviewed to see if there is any bias and identify options. After the gaps are identified and the data is analyzed, a presentation is given again.

## Python
[Titanic Dataset - Exploratory Analysis](https://github.com/goutam63/Data-analytics-/blob/main/Titanic%20Dataset%20-%20Exploratory%20Analysis.ipynb): Exploratory Analysis of the passengers onboard RMS Titanic using Pandas and Seaborn visualisations.	

[Stock Market Analysis for Tech Stocks](https://github.com/goutam63/Data-analytics-/blob/main/Stock%20Market%20Analysis%20for%20Tech%20Stocks.ipynb): Analysis of technology stocks including change in price over time, daily returns, and stock behaviour prediction.

[2016 US General Election Poll Data Analysis](https://github.com/goutam63/Data-analytics-/blob/main/2016%20General%20Election%20Poll%20Analysis.ipynb): Very simple analysis of 2016 US General Election Poll data.

[911 Calls - Exploratory Analysis](https://github.com/goutam63/Data-analytics-/blob/main/911%20Calls%20-%20Exploratory%20Analysis.ipynb): Exploratory Data Analysis of the 911 calls dataset hosted on Kaggle. Demonstrates extraction of useful features from different variables.
